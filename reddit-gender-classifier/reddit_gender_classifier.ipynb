{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4db9ddd-3487-4987-8cbc-8891bce8cf56",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e8de08fe-de9c-4998-91a6-d101e5fe0445",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections.abc import Iterable, Sequence\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import enum\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import sparse\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit, cross_val_score\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18210bed-9ba8-49da-b1a4-d6429cebbc0c",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "769dd38a-c0ab-4c93-81b7-f558bbfcda45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Utility Functions #\n",
    "#####################\n",
    "\n",
    "def get_dataframes_from_csv(path_to_features, path_to_targets=None):\n",
    "    \"\"\"\n",
    "    Get Pandas DataFrames for features and targets\n",
    "    \"\"\"\n",
    "    features_dataframe = pd.read_csv(path_to_features, encoding=\"utf8\")\n",
    "    \n",
    "    if not path_to_targets:\n",
    "        return features_dataframe\n",
    "    \n",
    "    targets_dataframe = pd.read_csv(path_to_targets)\n",
    "    return features_dataframe, targets_dataframe\n",
    "\n",
    "def get_features_from_dataframe(features_dataframe) -> tuple[list, list, list, list]:\n",
    "    \"\"\"\n",
    "    Get all feature columns as lists from DataFrame\n",
    "    \"\"\"\n",
    "    usernames = list(features_dataframe.author)\n",
    "    comments = list(features_dataframe.body)\n",
    "    subreddits = list(features_dataframe.subreddit)\n",
    "    created_at = list(features_dataframe.created_at)\n",
    "    return usernames, comments, subreddits, created_at\n",
    "\n",
    "def get_targets_from_dataframe(features_dataframe, targets_dataframe) -> list[int]:\n",
    "    \"\"\"\n",
    "    Get targets as lists from DataFrame\n",
    "    \"\"\"\n",
    "    targets_dictionary : dict[str, int] = dict(zip(targets_dataframe.author, \n",
    "                                                   targets_dataframe.gender))\n",
    "    targets : list[int] = list(map(lambda a: targets_dictionary[a], features_dataframe.author))\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "15ff3fe7-5e0d-4d60-ba2e-0faa56a4fc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I don't think we'd get nearly as much fanfiction and pictures shipping Ban-Ban and Lyro. Just saying.\", \"Thanks. I made it up, that's how I got over my first heart break. \", \"Are you sure you aren't confusing Cyclops (the easiest boss monster) for Ogres? I'm talking about [these guys](http://i.imgur.com/c3YKPdI.jpg)\\n\\nMaybe I'm just a bad player... But every time I faced one on my first playthrough, all my pawns ended up getting to 0 HP at least once and I could barely get an attack in once it started berserking.\"]\n",
      "[0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "#############\n",
    "# Execution #\n",
    "#############\n",
    "\n",
    "training_features_dataframe, training_targets_dataframe = get_dataframes_from_csv(\"data/train_data.csv\", \n",
    "                                                                             \"data/train_target.csv\")\n",
    "training_comments : list[str] = list(training_features_dataframe.body)\n",
    "training_targets : list[int] = get_targets_from_dataframe(training_features_dataframe, \n",
    "                                                          training_targets_dataframe)\n",
    "\n",
    "print(training_comments[:3])\n",
    "print(training_targets[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4bfc8a-0b65-4c1f-8b93-449af6c03abc",
   "metadata": {},
   "source": [
    "# Group by author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6c61ab8c-0af7-43b0-82d3-f92bab338c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Utility Functions #\n",
    "#####################\n",
    "\n",
    "def group_dataframe_by_author(features_dataframe):\n",
    "    \"\"\"\n",
    "    Group all features in the dataframe by author.\n",
    "    \"\"\"\n",
    "    return features_dataframe.groupby('author', as_index=False).agg({\n",
    "                         'subreddit':join_strings, \n",
    "                         'body':join_strings, \n",
    "                         'created_utc': join_ints})\n",
    "\n",
    "def join_strings(x : Iterable[str]):\n",
    "    \"\"\"\n",
    "    Join all elements of a list/iterable of strings with a white-space in-between.\n",
    "    \"\"\"\n",
    "    return ' '.join(x)\n",
    "\n",
    "def join_ints(x : Iterable[int]):\n",
    "    \"\"\"\n",
    "    Join all elements of a list/iterable of ints with a comma in-between.\n",
    "    \"\"\"\n",
    "    return ','.join(map(lambda i: str(i), x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "36df8edb-e200-4b2b-8b4f-58018040081c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           author                                          subreddit  \\\n",
      "0          -Jared   AskReddit tall pics StarWars AskReddit AskReddit   \n",
      "1         -Peeter                                             gainit   \n",
      "2        -evasian  MouseReview MechanicalKeyboards jailbreak jail...   \n",
      "3         -rubiks  AskWomen AskWomen AskWomen AskWomen AskWomen A...   \n",
      "4  -true_neutral-                    mildlyinteresting todayilearned   \n",
      "\n",
      "                                                body  \\\n",
      "0  Neil Diamond - Sweet Caroline +1 on the chirop...   \n",
      "1                         Just read the FAQ, really.   \n",
      "2  I just received my Deathadder Black Edition ye...   \n",
      "3  AlunaGeorge - Best Be Believing\\nArctic Monkey...   \n",
      "4  &gt; Urban Ears Metis\\n\\nYMMV. I have had thre...   \n",
      "\n",
      "                                         created_utc  \n",
      "0  1390189315.0,1390189970.0,1390492589.0,1390496...  \n",
      "1                                       1389962703.0  \n",
      "2  1388678755.0,1388688144.0,1389891805.0,1389892...  \n",
      "3  1389194620.0,1389500510.0,1389881305.0,1389882...  \n",
      "4                          1391042356.0,1391143961.0  \n",
      "['Neil Diamond - Sweet Caroline +1 on the chiropractor. I went religiously from ages 14 - 19 due to extreme lower back pain from running track / cross country. It\\'s important to remember it\\'s a somewhat slow process, but worth the results. \\n\\nOh, and don\\'t worry, they aren\\'t cracking your back - its just an \"adjustment\"  Looks like the pattern on the envelope that middle school photos came in.  If you had a bad motivator would you want to spell that out every time? And nothing beats a hangover quite like bacon (or better yet, Taylor ham), eggs over easy, toast, and hash browns! No, I think YOU mean Taylor Ham... Really, whatever you call it doesn\\'t matter, its damn delicious. ']\n",
      "[0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "#############\n",
    "# Execution #\n",
    "#############\n",
    "\n",
    "training_features_dataframe_groupby_author = group_dataframe_by_author(training_features_dataframe)\n",
    "\n",
    "training_comments_groupby_author : List[str] = list(training_features_dataframe_groupby_author.body)\n",
    "training_targets_groupby_author : List[int] = get_targets_from_dataframe(training_features_dataframe_groupby_author, training_targets_dataframe)\n",
    "\n",
    "print(training_features_dataframe_groupby_author.head())\n",
    "print(training_comments_groupby_author[:1])\n",
    "print(training_targets_groupby_author[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9f6a11-9673-4d0a-9994-3d2eb6ba3d94",
   "metadata": {},
   "source": [
    "# Clean textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d8c26334-6e21-4dec-a66a-785f3e3f8d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# Utility Classes #\n",
    "###################\n",
    "\n",
    "class TokenPatterns(enum.Enum):\n",
    "    \"\"\"\n",
    "    Exposes useful regex patterns for CountVectorizer and TfIdfVectorizer\n",
    "    \"\"\"\n",
    "    TKN_ALL_BUT_WHITESPACE = r'[^\\s]+'\n",
    "    \"\"\"Match all groups of chars separated by whitespaces\"\"\"\n",
    "\n",
    "    TKN_3_OR_MORE_ALPHA = '(?u)\\\\b[A-Za-z]{3,}'\n",
    "    \"\"\"Match whole words of alphabetical chars of three or more chars\"\"\"\n",
    "\n",
    "    TKN_1_OR_MORE_ALPHA = '(?u)\\\\b[A-Za-z]{1,}'\n",
    "    \"\"\"Match whole words of alphabetical chars of one or more chars\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c577fa89-564c-469d-a51c-181d89db3070",
   "metadata": {},
   "source": [
    "# Generate Part-Of-Speech data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6bbd9b1a-cb93-4bc2-88a5-78b9aa02a1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Utility Functions #\n",
    "#####################\n",
    "\n",
    "def get_pos_tag_count(text : str) -> dict[str, float]:\n",
    "    lower_case = text.lower()\n",
    "    tokens = nltk.word_tokenize(lower_case)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    counts = Counter(tag for word, tag in tags)\n",
    "    total = sum(counts.values(), 0.0)\n",
    "    normalized_counts = {k: v / total for k, v in counts.items()}\n",
    "    return normalized_counts\n",
    "\n",
    "###################\n",
    "# Utility Classes #\n",
    "###################\n",
    "\n",
    "class PosTagVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom vectorizer.\n",
    "    Transforms an iterable of textual data in a matrix of Part-Of-Speech counts.\n",
    "    Uses DictVectorizer by default.\n",
    "    \"\"\"\n",
    "    def __init__(self, vectorizer=None):\n",
    "        super().__init__()\n",
    "        if vectorizer:\n",
    "            self.vectorizer = vectorizer\n",
    "        else:\n",
    "            self.vectorizer = DictVectorizer()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.vectorizer.fit(map(get_pos_tag_count, X))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return self.vectorizer.transform(map(get_pos_tag_count, X))\n",
    "    \n",
    "    def get_feature_names_out(self):\n",
    "        return self.vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "085ad9ae-9b93-4df1-98ed-be7358e152b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.42857142857142855\n",
      "  (0, 1)\t0.42857142857142855\n",
      "  (0, 2)\t0.14285714285714285\n",
      "['.' 'NN' 'VBP']\n"
     ]
    }
   ],
   "source": [
    "#############\n",
    "# Execution #\n",
    "#############\n",
    "\n",
    "pos_vectorizer = PosTagVectorizer()\n",
    "print(pos_vectorizer.fit_transform([\"hi i am matteo!!!\"]))\n",
    "print(pos_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decf0c5f-4997-4cd3-b88c-2004f145c55b",
   "metadata": {},
   "source": [
    "# Generate subreddit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a911a192-c070-43f3-869c-d19778b5ab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Utility Functions #\n",
    "#####################\n",
    "\n",
    "def get_subreddit_count(subreddits : str) -> dict[str, int]:\n",
    "    subs = subreddits.split()\n",
    "    counts = Counter(subreddit for subreddit in subs)\n",
    "    return counts\n",
    "\n",
    "###################\n",
    "# Utility Classes #\n",
    "###################\n",
    "\n",
    "class SubredditVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom vectorizer.\n",
    "    Transforms a string of subreddits (separated by whitespace) in a matrix of Part-Of-Speech subreddit counts.\n",
    "    Uses DictVectorizer by default.\n",
    "    \"\"\"\n",
    "    def __init__(self, vectorizer=None):\n",
    "        super().__init__()\n",
    "        if vectorizer:\n",
    "            self.vectorizer = vectorizer\n",
    "        else:\n",
    "            self.vectorizer = DictVectorizer()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.vectorizer.fit(map(get_subreddit_count, X))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return self.vectorizer.transform(map(get_subreddit_count, X))\n",
    "    \n",
    "    def get_feature_names_out(self):\n",
    "        return self.vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0cb32166-386f-4f26-b321-2b6693ce3afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t5.0\n",
      "  (0, 1)\t1.0\n",
      "  (0, 2)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (1, 3)\t1.0\n",
      "['AskWomen' 'jailbreak' 'mildlyinteresting' 'motors']\n"
     ]
    }
   ],
   "source": [
    "#############\n",
    "# Execution #\n",
    "#############\n",
    "\n",
    "subreddit_vectorizer = SubredditVectorizer()\n",
    "print(subreddit_vectorizer.fit_transform([\"AskWomen AskWomen AskWomen AskWomen AskWomen mildlyinteresting jailbreak\", \"jailbreak motors\"]))\n",
    "print(subreddit_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ebcde3-de1b-436c-8707-c552cc5a5bd0",
   "metadata": {},
   "source": [
    "# Generate timestamp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb18f16-adc9-477c-b6af-0ec78ebce642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44bbd8ee-fa0c-4022-b504-6d829a5d1b75",
   "metadata": {},
   "source": [
    "# Generate username data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d574df66-b489-413d-8567-2b117f5b3c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab7dc5d2-7049-4135-8388-3abc2481ef57",
   "metadata": {},
   "source": [
    "# Generate CV-TFIDF comment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8630307f-6147-44f4-9c47-9c2ef414efb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22e84dec-804a-47d7-bafc-293f50102eb4",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160d596b-8770-4b65-bc4b-96af31479014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "898e922f-e42a-4a6f-9ab7-0f7936c62692",
   "metadata": {},
   "source": [
    "# Fit together/separately and with different estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a14dd2-1f47-457e-8b4e-5fe49468c858",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757f0be2-3c62-45c7-8864-b6d1569580d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3308234c-922d-48f8-bed4-65c8cbbdfed5",
   "metadata": {},
   "source": [
    "## Grid Search and Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0073b2-352d-4a19-bc4c-7753fe80b06c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c92d4ce8-627f-407e-a6d4-c32a7ddd26df",
   "metadata": {},
   "source": [
    "# Ensemble classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf3df5d-ee70-4e6f-b60f-6eb57df07b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5b86653-28e5-42a9-acb3-9766cc8e7432",
   "metadata": {},
   "source": [
    "# Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c0e6bb-3af6-4c44-a862-dddbe78bdfb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
